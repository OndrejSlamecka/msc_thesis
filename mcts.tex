\chapter{Monte Carlo Tree Search}
\label{ch_mcts}

Monte Carlo methods\footnote{Not to be confused with
Monte Carlo algorithms which are precisely defined as the algorithms
solving the decision problems in classes BPP and RP.} are using random sampling to estimate the correct
solution to a problem. The first serious use of Monte Carlo methods was
by Stanislaw Ulam and John von Neumann during their work on the
Manhattan project, but the technique has since spread into many areas of
science due to its general applicability.

One of the celebrated Monte Carlo methods
is the simulated annealing algorithm (so called due to its
origin in statistical physics) which is an improved version of
Metropolis algorithm (invented by a Manhattan project scientist
Nicholas C. Metropolis and others).

This method found its way into game
theory in 1993 when it was applied to the board game Go
\parencite{MonteCarloGo}. The approach was later further improved
\parencite{MonteCarloGoDevel} but the real breakthrough came in 2006
when Coulom \parencite{Coulom} and Kocsis, Szepesv√°ri \parencite{Kocsis}
independetly explored the idea of maintaing a tree which would guide the
search for strategies -- thus discovering Monte Carlo Tree Search.
This was eventually used in the AlphaGo program
\parencite{alphago}, the first
computer program to beat professional human players.

Monte Carlo Tree Search (MCTS) is, in short, a
heuristic search algorithm for finding good strategies in complex
decision processes by combining standard approaches of artificial
intelligence and computational statistics: tree search and sampling.

In this chapter the general MCTS scheme is defined and a concrete instance
called UCT is shown together with its application to maximizing rewards
in MDPs and games. The chapter is based mostly on a thorough MCTS survey
paper \parencite{mcts_survey}.

Since the research into MCTS focuses mainly on using MCTS for reward
maximization we demonstrate the algorithms on the reward maximization
problem too in this chapter unlike the rest of the thesis.

\section{General MCTS Scheme}

MCTS iteratively builds a tree which approximates possible resulting
rewards of strategies in the decision process. In each iteration the
tree guides the search to balance between exploitation of known good
strategies and exploration of new strategies. When the search leaves the
tree it proceeds at random and upon terminating it adds a new leaf to
the tree and updates its ancestors with the result. This is summarized
in \autoref{mcts}.


\begin{algorithm}
\caption{General Monte Carlo Tree Search method}
\label{mcts}
\begin{algorithmic}
\Function{MCTS}{$s_0$}
    \State Let $v_0$ be the root of the MCTS tree, with $v_0.state = s_0$.
    \While{within computational budget}
        \State $v_l \gets \Call{TreePolicy}{v_0}$
        \State $\Delta \gets \Call{Rollout}{v_l}$
        \State $\Call{Backup}{v_l, \Delta}$
    \EndWhile
    \State \Return Action from $v_0$ to the best node (by some
    given metric).
\EndFunction
\end{algorithmic}
\end{algorithm}

\section{Upper Confidence Bound for Trees}

The most common implementation of the general scheme is {\em Upper
Confidence Bound for Trees} (UCT) which utilizes formula \ref{UCB} to
select nodes in the tree traversal.
In this formula $\overline{X}_i$ represents the expected outcome
from node $i$, $n$ is the number of visits to all nodes, $n_i$ is the
number of visits to node $i$. $C$ is an arbitrary constant.

\begin{equation}
\label{UCB}
UCT_i = \overline{X}_i + C \sqrt{ \frac{2 \ln n}{n_i} }
\end{equation}

The tree is traversed greedily, in every step a node with maximal UCT
value is chosen. Note that unvisited nodes have their UCT value equal to $\infty$.
Importantly when a node $j$ is visited, its sibling's $i$ UCT value is
increased as $n$ increased but $n_i$ did not -- this contributes to
balance between exploitation of known good strategies and exploration of new.
Constant $C$ instructs the algorithm how much weight to give to
exploration. See \parencite{improved_mc} for insights into choice of $C$
in some specific situations.

\autoref{uct} is implementation of the general \autoref{mcts}. As a
minor simplification the algorithm expects an MDP with rewards
$(S,s_0,A,E,R)$, such that a reward is paid out only when a terminal
state is reached.

Every tree node $v$ has information about its corresponding state
($v.state$), the action which lead to it ($v.action$), the number of
times it has been visited so far ($v.visits$), and the reward which has
been collected after traversing through it ($v.q$, the Q refers to
Q-learning).

TODO: More explanation. e.g. why does it return what it returns,
what does q/n mean...

\begin{algorithm}
    \caption{Upper Confidence Bound for Trees}
\label{uct}
\begin{algorithmic}
\Function{UCT}{$s_0$}
    \State Let $v_0$ be the root of the MCTS tree, with $v_0.state = s_0$.
    \While{within computational budget}
        \State $v_l \gets \Call{TreePolicy}{v_0}$
        \State $r \gets \Call{Rollout}{v_l}$
        \State $\Call{Backup}{v_l, r}$
    \EndWhile
    \State \Return $\Call{BestChild}{v_0, 0}.action$
\EndFunction

\Function{TreePolicy}{v}
    \While{$v$ is not terminal}
        \If{$v$ is not fully expanded}
            \State $\Call{Expand}{v}$
        \Else
            \State $v \gets \Call{BestChild}{v, C}$
        \EndIf
    \EndWhile
\EndFunction

\Function{Expand}{v}
    \State choose an untried action $a$ from $A(v.state)$
    \State add a new child $v'$ to $v$,
        $v'.state = \Delta(v.state, a),
        v'.action = a$
\EndFunction

\Function{BestChild}{v, c}
    \State \Return
    $\argmax\limits_{v' \text{ a child of } v}
    \frac{v'.q}{v'.visits} +
    c\sqrt{\frac{2\ln v.visits}{v'.visits}}$
\EndFunction

\Function{Rollout}{s}
    \While{$s$ is not terminal}
        \State choose $a \in E(s)$ uniformly at random
        \State $s \gets \Delta(s,a)$
    \EndWhile
\EndFunction

\Function{Backup}{s}
    \While{$v$ is not null}
        \State $v.visits \gets v.visits + 1$
        \State $v.q \gets v.q + r$
        \State $v \gets$ parent of $v$
    \EndWhile
\EndFunction
\end{algorithmic}
\end{algorithm}

TODO: Example execution.

As UCT is the most common type of MCTS algorithm, the terms UCT and MCTS
are often used interchangeably in literature.

\section{Solving Games}

In this section a brief introduction to game theory is given, starting
with definitions of games, strategies and solutions to games. We
proceed by presenting the standard {\em minimax} algorithm, then showing
how MCTS can be used to solve games and how it compares with minimax.
Lastly we briefly compare the application of MCTS to Go and Chess.
Observing where it performs good and where it does not provides insight
the algorithm and it is a useful starting point for understanding the results
of evaluation in \autoref{ch_evaluation}.

For precise understanding of the model of games we present the following
definition.  The perfect-information in games corresponds to full
observability in MDP.  The reader can notice other similarities with
MDPs as well, for example games have states, actions and enabled
actions, as well as rewards. On the other hand the transition function
is deterministic in our definition.

\begin{definition}
    A {\em perfect-information extensive-form game}
    is a tuple $G = (N, S, s_0, F, A, E, \Delta, \rho, u)$,
    where
    \begin{itemize}
        \item $N \subseteq \mathbb{N}$ is a set of $n$ players,
            for $n \in \mathbb{N}$,
        \item $S$ is a set of states,
        \item $s_0 \in S$ is the initial state,
        \item $F \subseteq S$ is a set of terminal states,
        \item $A$ is the set of actions,
        \item $E : S \to \mathcal{P}^+(A)$ is a function which tells the player
            which non-empty set of actions can be played in a given state,
        \item $\Delta : (S \setminus F) \times A \to A$ is the
            transition function, defined such that game states cannot
            repeat\footnote{
                Technically there are no $s \in S, a \in E(s)$ such that
                $\Delta(s,a) = s_0$, and for any two $s,s' \in S$ there
                are no actions $a \in E(s), a' \in E(s')$ such that
                $\Delta(s,a) = \Delta(s',a')$.
            },
        \item $\rho : S \to N$ is the function which determines who plays in each state,
            and finally
        \item $u = (u_1,\ldots,u_{n})$ is the tuple of
            utility (payoff, reward) functions where each $u_i, i \in N$
            is a function $u_i : F \to \mathbb{R}$.
    \end{itemize}
\end{definition}

Game starts in state $s_0$ and players take turns until
a terminal state (in $F$) is reached. In a turn a player in state $s$
chooses action $a$ from $E(s)$ and the action leads to state
$\Delta(s,a)$.  If the state $\Delta(s,a)$ is terminal, then every
player $i$ receives the reward $u_i(\Delta(s,a))$ and the game ends.

\begin{example}
A common choice of the utility function is $+1, 0, -1$ for victory, draw
and loss, respectively. Below is a depiction of such a game
in the usual way -- using a tree.
There are two players,
the nodes represent game states, leafs are the final states.
The root of the tree is the initial state.
Actions are given implicitly by choice of the next node.
Functions $E$ and $\Delta$ are given by the drawn edges.
$\rho$ is given by the color of the nodes
and the payoff function is represented by a tuple in each leaf.

\begin{center}
\begin{tikzpicture}
\tikzset{
solid node/.style={circle,draw,inner sep=1.5,minimum size=10pt,fill=black},
hollow node/.style={circle,draw,inner sep=1.5,minimum size=10pt}
}

\node(0)[solid node]{}
    child{node[hollow node,label=below:{$(1,1)$}]{}}
    child{node[hollow node]{}
        child{node[solid node,label=below:{$(-1,1)$}]{}}
        child{node[solid node,label=below:{$(1,-1)$}]{}}
    };
\end{tikzpicture}
\end{center}
\end{example}


\begin{definition}
    A {\em strategy} of player $i$ in a game $G$ is a function
    $\pi_i : S \to \distribution{A}$, such that for all $s \in S$ and $a
    \in A$ it holds that if $\pi_i(s)(a) > 0$ then $a \in E(s)$.

    A {\em strategy profile} is an $N$-tuple with a strategy for each player.
\end{definition}

We assume the players are rational and thus pick strategies which
optimize for their goals. A goal may be to maximize the player's reward,
another goal may be to get a higher score than the opponents.

\begin{example}
    In the game shown below let Alice be player 1 and Bob player 2.
    There is a single turn made by Alice in the black node.
If her goal is to beat Bob she will pick the left node.
If her goal is to maximize her profit she will choose the right one.

\begin{center}
\begin{tikzpicture}
\tikzset{
solid node/.style={circle,draw,inner sep=1.5,minimum size=10pt,fill=black},
hollow node/.style={circle,draw,inner sep=1.5,minimum size=10pt}
}

\node(0)[solid node]{}
    child{node[hollow node,label=below:{$(2,1)$}]{}}
    child{node[hollow node,label=below:{(5,10)}]{}};
\end{tikzpicture}
\end{center}
\end{example}

In the following text
we restrict ourselves to two player games in which players alternate in
taking actions.

\subsection{Zero-sum games and minimax}

In zero-sum games one player wins at the cost of the other losing.
There are plenty of examples of such games, e.g. Chess, Go, which makes
them an important subject of study.

\begin{definition}
    Let $G = (N, S, s_0, F, A, E, \Delta, \rho, u)$ be a game.
    $G$ is said to be {\em zero-sum} if the rewards always sum to zero,
    that is $\sum_{i \in N} u_i(s) = 0$ for all $s \in F$.
\end{definition}

An important type of strategy is {\em minimax}. A player playing this
strategy is minimizing their potential maximum loss\footnote{Some might
prefer calling it {\em maximin} for maximization of the minimum reward,
which is equivalent to the first definition in zero-sum games.}. If both players play a
minimax strategy, the strategy profile is a Nash
equilibrium\footnote{
A strategic profile is a Nash equilibrium, if no player can get a higher
reward by switching a strategy.
    }$^,$\footnote{The {\em minimax theorem} was proven by John
von Neumann.}.

The {\em minimax algorithm} computes the value of each leaf in a game
tree assuming the opponent tries to harm the player as much as
possible. That is if player $1$ is computing which turn to play,
she uses the fact that she can maximize her profit in her future turns,
but assumes player $2$ will try to minimize it.
After all options are computed in this alternating manner,
player $1$ plays the action going into the subtree with the highest value node.

Now we use the zero-sum property and our assumption of alternating turns
to simplify the algorithm for zero-sum games in two steps.

First, only a single payoff function can be used,
let it be $u_1$. Now higher values mean player 1 is winning and lower
values that player 2 is winning.
Thus player 1 maximizes in her turns, expects
player 2 to minimize in his turns.
Player 2 minimizes in his turns, expects
player 1 to maximize in her turns.

Second, instead of alternating between maximization and minimization an algorithm
can always maximize if it alters the sign of the value in each step.

These simplifications result in \autoref{negamax}, the {\em negamax
algorithm}.
Player 1 gets the value by evaluating
\textsc{Negamax}$($state$, 0)$,
player 2 gets the value by evaluating $-$\textsc{Negamax}$($state$, 1)$.

An example execution of the algorithm can be seen on a game of
tic-tac-toe in \autoref{fig:negamax}.
X is the first player, O is the second player. At the beginning X can
place her mark in the middle of the board and win, or place it at the
bottom and lose. The left branch has value 1, the right branch has value
$-1$.

% I read about the topic here:
% http://www.hamedahmadi.com/gametree/#negamax
% https://en.wikipedia.org/wiki/Minimax
% https://github.com/soratobukuroneko/tictactoe-negamax/blob/master/tictactoe.c
% https://neverstopbuilding-dropblog.herokuapp.com/minimax

\begin{algorithm}
\caption{Negamax}
\label{negamax}
\begin{algorithmic}
\Input Two-player zero-sum game\footnote{The $player$
numbers are shifted down by one for easy manipulation.}
$(N, S, s_0, F, A, E, \Delta, \rho, u)$, where players
 alternate in taking actions.
\Output Maximum payoff for player 0, its negation for player 1.
\Function{negamax}{$state, player$}
    \If{$state \in F$, i.e. a leaf in the game tree}
    \State \Return\footnote{The
    $[1,-1][player]$ evaluates to 1 for player 0 and to $-1$ for player 1.}
        $[1,-1][player] \cdot u_1(state)$
        otherwise
    \EndIf
    \State \Return $\max \{ - \Call{negamax}{child, 1 - player}
        \mid
        child \text{ of } state\footnote{Precisely
        for all enabled actions $a \in E(state)$ every $child \in \Delta(state, a)$.}
    \}$
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{figure}
% https://tex.stackexchange.com/questions/139782/creating-tic-tac-toe-boards-with-latex-tikz
\begin{forest}
  TTT/.style args={#1:#2}{
    make tab/.expanded=\forestove{content},
    label={\pgfkeysvalueof{/forest/label position #1}:$#2$}
  },
  TTT*/.style={
    make tab=::/::/::,
    content/.expand once=%
    \expandafter\vphantom\expandafter{\romannumeral-`0\forestov{content}},
    draw=none,
    append after command={(\tikzlastnode.north) edge (\tikzlastnode.south)},
    for descendants={before computing xy={l*=1.2}},
  },
  th/.style=thick,
  for tree={
    node options=draw,
    inner sep=+0pt,
    parent anchor=south,
    child anchor=north,
    l sep=0.7cm,
    s sep=1.7cm,
  },
  levellabel/.style={
    tikz+={
      \node [anchor=mid west] at (.mid -| levellabel coord) {#1};
    },
  },
  tikz+={
    \coordinate (levellabel coord) at (current bounding box.east);
  },
%
[o:o:x/x::o/x::o, TTT=r:{}, levellabel={\text{x's turn}, player = 0},
 [o:o:x/x:x:o/x::o, TTT=b:{\makecell[c]{(-1)\cdot 1 \\ \text{x wins}}},
   edge label={node[midway,left,font=\scriptsize]{$\cdot (-1)$}}]
   [o:o:x/x::o/x:x:o, TTT=r:{}, levellabel={\text{o's turn}, player = 1},
   edge label={node[midway,right,font=\scriptsize]{$\cdot (-1)$}},
   [o:o:x/x:o:o/x:x:o, TTT=b:{\makecell[c]{1\cdot (-1) \\ \text{o wins}}},
   levellabel={\phantom{\text{o's turn}, }player = 0},
   edge label={node[midway,right,font=\scriptsize]{$\cdot (-1)$}}]
 ]
]
\end{forest}
\caption{Negamax algorithm applied to tic-tac-toe.}
\label{fig:negamax}
\end{figure}

For larger games a variation called {\em minimax (negamax) search} is used, which
performs the computation only to a limited depth, and then uses
a heurisic to evaluate the last node it processes (unless it is a leaf).

\subsection{Solving with MCTS}

From the negamax algorithm in the previous section there is an easy step
to solving games with MCTS. \autoref{alg:mcts_negamax} is an
implementation of the Backup function in UCT for finding a good
action a player should take in a zero-sum game.

\begin{algorithm}
\caption{Negamax MCTS Backup}
\label{alg:mcts_negamax}
\begin{algorithmic}
\Function{backup}{$v, \Delta$}
    \While{$v$ is not $null$}
        \State $N(v) \gets N(v) + 1$
        \State $Q(v) \gets Q(v) + \Delta$
        \State $\Delta \gets - \Delta$
        \State $v \gets $ parent of $v$
    \EndWhile
\EndFunction
\end{algorithmic}
\end{algorithm}

% http://tim.hibal.org/blog/alpha-zero-how-and-why-it-works/

In comparison with minimax search MCTS may ignore unimportant
branches of the game tree and instead explore in greater detail the more
promising branches.

For example when solving a tic-tac-toe game the algorithm starts with
the current board state in the root of the tree and then proceeds to
add the immediate turns into the tree. They are explored with random
rollouts and the algorithm proceeds to the parts of tree which
either seem to lead victory or which are unsufficiently explored, the
balance between exploitation and exploration is dependent on the choice of
the constant in the tree heuristic. See \parencite{mcts_ttt} for an
elaborate explanation with illustrations.

An important theoretical result is
that MCTS converges to minimax \parencite{Kocsis},
so eventually, after a lot of MCTS iterations, there is a negligible
difference between the results of the two methods and they pick the same
next move. However in
applications we want to know when to use minimax (\autoref{negamax}) and
when to use UCT (\autoref{uct} with \autoref{alg:mcts_negamax}) without
iterating UCT for a long time.

While UCT achieved success in many games \parencite{mcts_survey},
for example in chess it does not perform well. The reason being that chess
games contain a lot of trap states (states from which the opponent
has a guaranteed victory) which are reachable just in a few turns. UCT
then spends a lot of time exploring these unimportant parts of the search
tree \parencite{mcts_vs_chess}.

\subsection{Computer Go}

Go is an example of a game where MCTS has proven to be a good choice,
most recently with the AlphaGo program \parencite{alphago}.
We shortly introduce Go and compare it with chess where MCTS did not
achieve success.

The basic rules of Go are simple. The
standard board is a $9\times9$ (for beginners) or $19 \times 19$ grid.
Two players, black and white, alternate in their moves, each
placing a single stone of their color on an intersection of lines.
By surrounding the stones of the opponent a player captures all the
surrounded stones. The game ends when both players agree to end and the
winner is the player with greater sum of controlled territory and
captured stones.
The rules are well explained in detail at
\href{http://playgo.to/iwtg/en/}{http://playgo.to/iwtg/en/}.

A Go player can observe that unlike in Chess there is seldom a quick way
to lose, as a loss of a small part of territory may be reversed, however
losing an important piece is hardly reversible.
This gives us an important example of games where MCTS performs well (in
Go it is worth exploring a plethora of moves) and where it does not
(e.g. in Chess, due to reasons described above).
It seems plausible this
generalizes well to territory versus piece based games.

We end our detour to games with Arimaa, a game specifically designed in
2002 to be easy for humans but hard for computers, for example by
allowing more moves per turn. Eventually Arimaa players lost to a
program in 2015 \parencite{arimaa}. The program uses techniques similar
to chess programs like alpha-beta pruning and on top of that employs
heuristics inspired by the best human players. See \parencite{jakl} for
MCTS related insights into Arimaa.

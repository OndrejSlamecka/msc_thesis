\chapter{MCTS in MDP Verification}

In this chapter three new algorithms for verification of Markov
decision processes are presented. One called
called Bounded MCTS (BMCTS) follows closely the general MCTS scheme
but the updates maintain lower and upper bounds. The second called
MCTS-BRTDP is a fusion of MCTS, where each rollout (TODO: make use of
this word consistent) is a single iteration of BRTDP.
The third one called is called BRTDP-UCB and uses the UCB formula to
select the next action in BRTDP.

These algorithms can be altered by changing the tree node selection
heuristic. We suggest and evaluate two formulas, UCB and VCB. Here $U_i$
is the known upper bound (of the sought probability) in the state
corresponding to the tree node $i$, $n_i$ is the number of visits of the
node (TODO: not state?) and $n$ is the number of iterations (visits of
the root node).
\begin{equation*}
    UCB_i = U_i + C \sqrt{ \frac{2 \ln n}{n_i} }
\end{equation*}

In the VCB (Victory Confidence Bound), $v_i$ is the number of times a
target state has been hit when rolling out from state $i$ or its child.
TODO: However experiments show its performance is similar to UCB.
\[
    VCB_i = \frac{v_i}{n_i} + C \sqrt{ \frac{2 \ln n}{n_i} }
\]

Furthermore we experimented with a formula which would utilize our
confidence in the learned upper bound but experiments show its
performance is very bad (TODO: check this is the case on more models).
\[
    CCB_i = U_i (1 - (U_i - L_i)) + C \sqrt{ \frac{2 \ln n}{n_i} }
\]


\section{Bounded MCTS}

BMCTS is an algorithm which implements the general MCTS scheme in the
most straightforard way while maintaining guarantees about the result.
The tree node selection is done using the
heuristics described above, in each step of each rollout the action is
chosen uniformly at random.

The main distinction is in the update phase. To maintain the lower and
upper bounds, backpropagation using the Bellman update is performed
from the final state of the rollout to the state corresponding to
the selected tree node, and then on the states corresponding to the
nodes on the taken tree path. Compare this with the usual MCTS
implementation where this is not necessary and updates are performed
only on the tree path.

This algorithm is an improvement upon random sampling as it guides the
random rollouts towards the parts of the decision process which seem
more relevant for maximizing the sought probability.

\begin{algorithm}
\caption{BMCTS}
\label{alg_bmcts}
\begin{algorithmic}
\Function{BMCTS}{$s_0$}
    \State Let $v_0$ be the root of the MCTS tree, with $v_0.state = s_0$.
    \While{$U(s_0) - L(s_0) > \epsilon$}
        \State $v_l \gets \Call{TreePolicy}{v_0}$
        \State $\Delta \gets \Call{DefaultPolicy}{v_l}$
        \State $\Call{Backup}{v_l, \Delta}$
    \EndWhile
    \State \Return Action from $v_0$ to the best node (by some
    given metric).
\EndFunction

\Function{DefaultPolicy}{$s$}
\EndFunction

\Function{TreePolicy}{$s$}
\Repeat
\Until{$s \neq s_0$}
\EndFunction

\Function{Backup}{$s_0, s$}
\Repeat
    \State $U(s,a) \coloneqq \sum_{s' \in S} \Delta(s,a)(s')U(s')$
    \State $L(s,a)\, \coloneqq \sum_{s' \in S} \Delta(s,a)(s')L(s')$
    \State $s \gets parent(s)$
\Until{$s \neq s_0$}
\EndFunction
\end{algorithmic}
\end{algorithm}


TODO: Prove AC.


\section{MCTS-BRTDP}

MCTS-BRTDP is a variation of BMCTS in which each rollout is a
single iteration of BRTDP as described in \autoref{ch_mdp}. This allows
for significantly faster updates as compared to BMCTS but still
utilizing exploration which makes it possible to overcome parts of the
MDP which are hard for BRTDP.

TODO: How do we deal with end components and why we don't do it. What if
we grow a tree into an end component? What happens if a state already is
in a tree?

The functions \textsc{TreePolicy} and \textsc{Backup} are implemented in
the same way as in \autoref{alg_bmcts}.


\begin{algorithm}
\caption{MCTS-BRTDP}
\label{mcts-brtdp}
\begin{algorithmic}
\Function{MCTS-BRTDP}{$s_0$}
    \State Let $v_0$ be the root of the MCTS tree, with $v_0.state = s_0$.
    \While{$U(s_0) - L(s_0) > \epsilon$}
        \State $v_l \gets \Call{TreePolicy}{v_0}$
        \State $\Delta \gets \Call{BRTDP}{v_l}$ % TODO: ITERATION
        \State $\Call{Backup}{v_l, \Delta}$
    \EndWhile
    \State \Return Action from $v_0$ to the best node (by some
    given metric).
\EndFunction

\end{algorithmic}
\end{algorithm}

TODO: Prove AC.


\section{UCB in BRTDP}

We further present one algorithm which is not based on MCTS but
incorporates the $UCB$ exploration term into BRTDP. TODO: pseudocode?
probably not, it is just BRTDP where the action is chosen according to
UCB, is it AC? can we prove it?

\chapter{MCTS in MDP Verification}

In this chapter three new algorithms for verification of Markov
decision processes are presented. One called
called Bounded MCTS (BMCTS) follows closely the general MCTS scheme
but the updates maintain lower and upper bounds. The second called
MCTS-BRTDP is a fusion of MCTS, where each rollout (TODO: make use of
this word consistent) is a single iteration of BRTDP.
The third one called is called BRTDP-UCB and uses the UCB formula to
select the next action in BRTDP.

These algorithms can be altered by changing the tree node selection
heuristic. We suggest and evaluate two formulas, UCB and VCB. Here $U_i$
is the known upper bound (of the sought probability) in the state
corresponding to the tree node $i$, $n_i$ is the number of visits of the
node (TODO: not state?) and $n$ is the number of iterations (visits of
the root node).
\begin{equation*}
    UCB_i = U_i + C \sqrt{ \frac{2 \ln n}{n_i} }
\end{equation*}

In the VCB (Victory Confidence Bound), $v_i$ is the number of times a
target state has been hit when rolling out from state $i$ or its child.
TODO: However experiments show its performance is similar to UCB.
\[
    VCB_i = \frac{v_i}{n_i} + C \sqrt{ \frac{2 \ln n}{n_i} }
\]

Furthermore we experimented with a formula which would utilize our
confidence in the learned upper bound but experiments show its
performance is very bad (TODO: check this is the case on more models).
\[
    CCB_i = U_i (1 - (U_i - L_i)) + C \sqrt{ \frac{2 \ln n}{n_i} }
\]


\section{MCTS-BRTDP}

MCTS-BRTDP is a variant of MCTS where the tree policy is the standard
selection of a node with the maximum UCB value. The default policy is
BRTDP as described in \autoref{ch_mdp} and the bound updates then continue
through the tree upwards to its root.

\begin{algorithm}
\caption{MCTS-BRTDP}
\label{mcts-brtdp}
\begin{algorithmic}
\Function{MCTS-BRTDP}{$s_0$}
    \State Let $v_0$ be the root of the MCTS tree, with $v_0.state = s_0$.
    \While{$U(s_0) - L(s_0) > \epsilon$}
        \State $v_l \gets \Call{TreePolicy}{v_0}$
        \State $\Delta \gets \Call{BRTDP}{v_l}$
        \State $\Call{Backup}{v_l, \Delta}$
    \EndWhile
    \State \Return Action from $v_0$ to the best node (by some
    given metric).
\EndFunction

\Function{TreePolicy}{$s$}
\Repeat
\Until{$s \neq s_0$}
\EndFunction

\Function{Backup}{$s_0, s$}
\Repeat
    \State $U(s,a) \coloneqq \sum_{s' \in S} \Delta(s,a)(s')U(s')$
    \State $L(s,a)\, \coloneqq \sum_{s' \in S} \Delta(s,a)(s')L(s')$
    \State $s \gets parent(s)$
\Until{$s \neq s_0$}
\EndFunction
\end{algorithmic}
\end{algorithm}

TODO: Prove AC.

\section{Bounded MCTS}

BMCTS is an algorithm similar to MCTS-BRTDP but the default policy
selects the next node (TODO: action?) at random with uniform
distribution.

TODO: Describe the advantage and disadvantages.

The functions \textsc{TreePolicy} and \textsc{Backup} are implemented in
the same way as in \autoref{mcts-brtdp}.

\begin{algorithm}
\caption{BMCTS}
\label{bmcts}
\begin{algorithmic}
\Function{BMCTS}{$s_0$}
    \State Let $v_0$ be the root of the MCTS tree, with $v_0.state = s_0$.
    \While{$U(s_0) - L(s_0) > \epsilon$}
        \State $v_l \gets \Call{TreePolicy}{v_0}$
        \State $\Delta \gets \Call{DefaultPolicy}{v_l}$
        \State $\Call{Backup}{v_l, \Delta}$
    \EndWhile
    \State \Return Action from $v_0$ to the best node (by some
    given metric).
\EndFunction

\Function{DefaultPolicy}{$s$}
\EndFunction

\end{algorithmic}
\end{algorithm}


TODO: Prove AC.


\section{UCB in BRTDP}

We further present one algorithm which is not based on MCTS but
incorporates the $UCB$ exploration term into BRTDP. TODO: pseudocode?
probably not, it is just BRTDP where the action is chosen according to
UCB, is it AC? can we prove it?

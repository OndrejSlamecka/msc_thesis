\chapter{MCTS in MDP Verification}

In this chapter three new algorithms for verification of Markov decision
processes are presented. One called follows
closely the general MCTS scheme but the updates maintain lower and upper
bounds. The second, is a fusion of MCTS and BRTDP, where each
rollout is a single iteration of BRTDP.  The third one
uses the UCB formula to select the next action in BRTDP.

Before proceeding, recall from \autoref{sec:brtdp} that $L,U : S \times
A \to [0,1]$, where $S, A$ are the sets of states and actions of an MDP,
are the lower and upper bounds of the value function $V$.
The section also defined
$U(s) = \max_{a \in E(s)} U(s, a)$
and
$L(s) = \max_{a \in E(s)} L(s, a)$.

The algorithms we suggest can be altered by changing the tree node
selection heuristic. We suggest and evaluate three formulas: UCB, VCB,
CCB.  Here $n_i$ is the number of visits of the node, $n$ is the
number of iterations (visits of the root node), $s_i$ is the number of
visits of the state corresponding to node $i$, and $s_r$ is the number
of visits to the initial state. $s_i/s_r$ thus represents a
guess\footnote{A biased guess as we update $s_j$ in tree traversal too.}
of the probability of reaching the state corresponding to node $i$.
\begin{equation*}
    UCB(i) = \frac{s_i}{s_r} \cdot U(i) + C \sqrt{ \frac{2 \ln n}{n_i} }
\end{equation*}

In the VCB (Victory Confidence Bound), $v_i$ is the number of times a
target state has been hit when rolling out from state $i$ or its child.
However experiments show its performance is similar to UCB in
practice or worse on some examples.
\[
    VCB(i) = \frac{v_i}{n_i} + C \sqrt{ \frac{2 \ln n}{n_i} }
\]

Furthermore we experimented with a formula which would utilize our
confidence in the learned upper bound but found out that on our models
its performance is very bad.
\[
    CCB(i) = U(i) (1 - (U(i) - L(i))) + C \sqrt{ \frac{2 \ln n}{n_i} }
\]

\pagebreak

\section{Bounded MCTS}

BMCTS, \autoref{alg:bmcts}, is an algorithm which implements the general
MCTS scheme in the most straightforward way while guaranteeing
approximate correctness of the result.

Function \textsc{TreePolicy} is implemented as in
\autoref{alg:uct} and uses one of the formulas
described at the beginning of this chapter. It starts at the root, and
then repeatedly chooses a child with the highest value given by a
formula until it reaches a leaf $v_l$. If the corresponding state
$v_l.state$ has successors, they are added in nodes as children of the
node and one of them is returned. If not, then $v_l$ is returned.

In each step of a rollout, an action is chosen
uniformly at random, and a state is then sampled according to the
transition distribution.

The primary distinction is in the backpropagation phase. To maintain the lower
and upper bounds, backpropagation using the Bellman update is performed
first in the MDP (\textsc{Backup}), and then in the tree (\textsc{TreeBackup}).

\textsc{Backup}: Perform Bellman updates from the terminal state reached in the
rollout to the state corresponding to the selected tree node. This is
the same procedure as in BRTDP.

\textsc{TreeBackup}: Perform Bellman updates from the states
corresponding to the nodes on the taken tree path. Here the algorithm
updates the bounds for every action as the tree does not differentiate
between actions.

Compare this backpropagation with a typical MCTS implementation where
this is not necessary and updates are performed only on the tree path.


This algorithm is an improvement upon random sampling as it guides the
random rollouts towards the parts of the decision process which look
more relevant to maximizing the sought probability. However, the random
rollouts prove to be ineffective in practice, which leads us to our main
algorithm.

\newpage

\begin{algorithm}[H]
\caption{BMCTS}
\label{alg:bmcts}
\begin{algorithmic}[1]
\Function{BMCTS}{$s_0$}
    \State Let $v_0$ be the root of the MCTS tree, with $v_0.state = s_0$.
    \While{$U(s_0) - L(s_0) > \epsilon$}
        \State $v_l \gets \Call{TreePolicy}{v_0}$
        \State $\omega \gets \Call{Rollout}{v_l.state}$
        \State $\Call{Backup}{\omega}$
        \State $\Call{TreeBackup}{v_l}$
    \EndWhile
    \State \Return $(L(s_0), U(s_0))$
\EndFunction

\Function{Rollout}{s}
    \State $\omega \gets s$
    \While{$last(\omega)$ is not terminal}
        \State $a \gets$ sample uniformly from $E(s)$
        \State $s \gets$ sample according to $\Delta(last(\omega),a)$
        \State $\omega \gets \omega \; a \; s$
    \EndWhile
    \State \Return $\omega$
\EndFunction

\Function{Backup}{$\omega$}
\While{$\omega$ is not empty}
    \State $pop(\omega)$
    \State $a \gets pop(\omega)$
    \State $s \gets pop(\omega)$
    \State $U(s,a) \coloneqq \sum_{s' \in S} \Delta(s,a)(s')U(s')$
    \State $L(s,a)\, \coloneqq \sum_{s' \in S} \Delta(s,a)(s')L(s')$
\EndWhile
\EndFunction

\Function{TreeBackup}{$v$}
\State $v \gets v.parent$ \Comment{The leaf's update was done in \textsc{Backup}}
\While{$v \neq v_0$}
    \State $s \gets v.state$
    \For{$a \in E(s)$}
        \State $U(s,a) \coloneqq \sum_{s' \in S} \Delta(s,a)(s')U(s')$
        \State $L(s,a)\, \coloneqq \sum_{s' \in S} \Delta(s,a)(s')L(s')$
    \EndFor
    \State $v \gets v.parent$
\EndWhile
\EndFunction
\end{algorithmic}
\end{algorithm}


\section{MCTS-BRTDP}

MCTS-BRTDP is a variation of BMCTS in which each rollout is a
single iteration of BRTDP as described in \autoref{ch_mdp}. This allows
for significantly faster updates of the bounds as compared to BMCTS but
still utilizing exploration which makes it possible to overcome parts of
the MDP which are hard for BRTDP.

The functions \textsc{TreePolicy} and \textsc{TreeBackup} are
implemented in the same way as in \autoref{alg:bmcts}. Recall that the
tree is traversed greedily by choosing uniformly one of the nodes with
maximal heuristic value. Tree backpropagation updates bounds on the
states corresponding to the nodes on the traversed path. Note that we
avoid using \textsc{Backup} as that is done by BRTDP.

Moreover, the implementation of BRTDP used here maintains a map from
states to lists of nodes in the tree. Every time a MEC
is collapsed in BRTDP, we remove the subtrees in the MCTS tree where the root node
corresponds to a state of the collapsed MEC.

This ensures that the algorithm maintains a consistent representation of
the MDP throughout the computation. Since MCTS does not restrict BRTDP
access to any part of the MDP it follows that the algorithm almost
surely converges to the correct value.

\begin{algorithm}
\caption{MCTS-BRTDP}
\label{mcts-brtdp}
\begin{algorithmic}[1]
\Function{MCTS-BRTDP}{$s_0$}
    \State Let $v_0$ be the root of the MCTS tree, with $v_0.state = s_0$.
    \While{$U(s_0) - L(s_0) > \epsilon$}
        \State $v_l \gets \Call{TreePolicy}{v_0}$
        \State an iteration of $\Call{BRTDP}{v_l}$
        \State $\Call{TreeBackup}{v_l}$
    \EndWhile
    \State \Return $(L(s_0), U(s_0))$
\EndFunction

\end{algorithmic}
\end{algorithm}

\pagebreak

\section{UCB in BRTDP}

We further present one algorithm which is not based on MCTS but
incorporates the UCB exploration formula into BRTDP. Instead of choosing
an action with the greatest upper bound, it chooses an action with the
greatest UCB value.

Line 7 of \autoref{alg:brtdp} is thus replaced with the line below,
where $s$ is the current state of the simulation ($last(\omega)$
in BRTDP). The range of the argmax are all states $i$ which can be
reached from $s$ through an allowed action $a$ with non-zero
probability. Here $n_i$ is the number of visits of state $i$,
and $n$ denotes the number BRTDP iterations run so far.

\bigskip
\begin{algorithmic}
    \State $a \gets$ sample uni. from
    $\argmax\limits_{a \in E(s), \forall i \text{ s.t. } \Delta(s,a)(i) > 0}
        U(i) + C \sqrt{ \frac{2 \ln n}{n_i} }$
\end{algorithmic}
\bigskip


This algorithm eventually explores as much as the previously mentioned
variants of BRTDP if necessary.



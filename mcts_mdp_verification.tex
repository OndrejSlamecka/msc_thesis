\chapter{MCTS in MDP Verification}

In this chapter three new algorithms for verification of Markov decision
processes are presented. One called called Bounded MCTS (BMCTS) follows
closely the general MCTS scheme but the updates maintain lower and upper
bounds. The second called MCTS-BRTDP is a fusion of MCTS, where each
rollout is a single iteration of BRTDP.  The third one called is called
BRTDP-UCB and uses the UCB formula to select the next action in BRTDP.

These algorithms can be altered by changing the tree node selection
heuristic. We suggest and evaluate two formulas, UCB and VCB. Here $U_i$
is the known upper bound (of the sought probability) in the state
corresponding to the tree node $i$, $n_i$ is the number of visits of the
node and $n$ is the number of iterations (visits of
the root node).
\begin{equation*}
    UCB_i = U_i + C \sqrt{ \frac{2 \ln n}{n_i} }
\end{equation*}

In the VCB (Victory Confidence Bound), $v_i$ is the number of times a
target state has been hit when rolling out from state $i$ or its child.
TODO: However experiments show its performance is similar to UCB.
\[
    VCB_i = \frac{v_i}{n_i} + C \sqrt{ \frac{2 \ln n}{n_i} }
\]

Furthermore we experimented with a formula which would utilize our
confidence in the learned upper bound but experiments show its
performance is very bad (TODO: check this is the case on more models).
\[
    CCB_i = U_i (1 - (U_i - L_i)) + C \sqrt{ \frac{2 \ln n}{n_i} }
\]

\pagebreak

\section{Bounded MCTS}

BMCTS is an algorithm which implements the general MCTS scheme in the
most straightforard way while maintaining guarantees about the result.
The tree node selection is done using the
heuristics described above, in each step of each rollout the action is
chosen uniformly at random.

The main distinction is in the update phase. To maintain the lower and
upper bounds, backpropagation using the Bellman update is performed
from the final state of the rollout to the state corresponding to
the selected tree node, and then on the states corresponding to the
nodes on the taken tree path. Compare this with the usual MCTS
implementation where this is not necessary and updates are performed
only on the tree path.

This algorithm is an improvement upon random sampling as it guides the
random rollouts towards the parts of the decision process which seem
more relevant for maximizing the sought probability.

\begin{algorithm}
\caption{BMCTS}
\label{alg_bmcts}
\begin{algorithmic}
\Function{BMCTS}{$s_0$}
    \State Let $v_0$ be the root of the MCTS tree, with $v_0.state = s_0$.
    \While{$U(s_0) - L(s_0) > \epsilon$}
        \State $v_l \gets \Call{TreePolicy}{v_0}$
        \State $\Delta \gets \Call{Rollout}{v_l}$
        \State $\Call{Backup}{v_l, \Delta}$
    \EndWhile
    \State \Return Action from $v_0$ to the best node (by some
    given metric).
\EndFunction

\Function{Rollout}{$s$}
TODO
\EndFunction

\Function{TreePolicy}{$s$}
\Repeat
\Until{$s \neq s_0$}
\EndFunction

\Function{Backup}{$s_0, s$}
\Repeat
    \State $U(s,a) \coloneqq \sum_{s' \in S} \Delta(s,a)(s')U(s')$
    \State $L(s,a)\, \coloneqq \sum_{s' \in S} \Delta(s,a)(s')L(s')$
    \State $s \gets parent(s)$
\Until{$s \neq s_0$}
\EndFunction
\end{algorithmic}
\end{algorithm}

\section{MCTS-BRTDP}

MCTS-BRTDP is a variation of BMCTS in which each rollout is a
single iteration of BRTDP as described in \autoref{ch_mdp}. This allows
for significantly faster updates as compared to BMCTS but still
utilizing exploration which makes it possible to overcome parts of the
MDP which are hard for BRTDP.

The functions \textsc{TreePolicy} and \textsc{Backup} are implemented in
the same way as in \autoref{alg_bmcts}. Furthermore a map from states to
lists of nodes in the tree is maintained and each time a MEC is
collapsed, the subtrees in the MCTS tree corresponding to the nodes
representing a state in the MEC are removed.

\begin{algorithm}
\caption{MCTS-BRTDP}
\label{mcts-brtdp}
\begin{algorithmic}
\Function{MCTS-BRTDP}{$s_0$}
    \State Let $v_0$ be the root of the MCTS tree, with $v_0.state = s_0$.
    \While{$U(s_0) - L(s_0) > \epsilon$}
        \State $v_l \gets \Call{TreePolicy}{v_0}$
        \State $\Delta \gets \text{an iteration of } \Call{BRTDP}{v_l}$
        \State $\Call{Backup}{v_l, \Delta}$
    \EndWhile
    \State \Return $(L(s_0), U(s_0))$
\EndFunction

\end{algorithmic}
\end{algorithm}



TODO:
is this almost surely correct algorithm?
I would say it is, as we know BRTDP is almost surely correct, MCTS eventually explores everything and the MCTS tree has always correct view of the actual MDP. So then it falls to the correctness of the update, which is the Bellman update, so it is ok.

\section{UCB in BRTDP}

We further present one algorithm which is not based on MCTS but
incorporates the $UCB$ exploration term into BRTDP. Instead of choosing
an action with the greatest upper bound, it chooses the action with the
greatest UCB value.

This algorithm eventually explores as much as the previously mentioned
variants of BRTDP if necessary.

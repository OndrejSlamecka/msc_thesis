\chapter{MCTS in MDP Verification}

In this chapter three new algorithms for verification of Markov decision
processes are presented. One called Bounded MCTS (BMCTS) follows
closely the general MCTS scheme but the updates maintain lower and upper
bounds. The second, MCTS-BRTDP, is a fusion of MCTS, where each
rollout is a single iteration of BRTDP.  The third one is called
BRTDP-UCB and uses the UCB formula to select the next action in BRTDP.

These algorithms can be altered by changing the tree node selection
heuristic. We suggest and evaluate two formulas, UCB and VCB. Here $U_i$
is the known upper bound (of the sought probability) in the state
corresponding to the tree node $i$, $n_i$ is the number of visits of the
node and $n$ is the number of iterations (visits of
the root node).
\begin{equation*}
    UCB_i = U_i + C \sqrt{ \frac{2 \ln n}{n_i} }
\end{equation*}

In the VCB (Victory Confidence Bound), $v_i$ is the number of times a
target state has been hit when rolling out from state $i$ or its child.
However experiments show its performance is similar to UCB in
practice or worse on some examples.
\[
    VCB_i = \frac{v_i}{n_i} + C \sqrt{ \frac{2 \ln n}{n_i} }
\]

Furthermore we experimented with a formula which would utilize our
confidence in the learned upper bound but found out that on our models
its performance is very bad.
\[
    CCB_i = U_i (1 - (U_i - L_i)) + C \sqrt{ \frac{2 \ln n}{n_i} }
\]

\pagebreak

\section{Bounded MCTS}

BMCTS is an algorithm which implements the general MCTS scheme in the
most straightforard way while maintaining guarantees about the result.
The tree node selection is done using the
heuristics described above, in each step of each rollout the action is
chosen uniformly at random.

The main distinction is in the update phase. To maintain the lower and
upper bounds, backpropagation using the Bellman update is performed
from the final state of the rollout to the state corresponding to
the selected tree node, and then on the states corresponding to the
nodes on the taken tree path. Compare this with the usual MCTS
implementation where this is not necessary and updates are performed
only on the tree path.

Functions \textsc{TreePolicy} and \textsc{Rollout} are implemented as in
\autoref{alg:uct}. \textsc{TreePolicy} uses one of the formulas
described at the beginning of this chapter.

This algorithm is an improvement upon random sampling as it guides the
random rollouts towards the parts of the decision process which look
more relevant for maximizing the sought probability.

\begin{algorithm}
\caption{BMCTS}
\label{alg_bmcts}
\begin{algorithmic}
\Function{BMCTS}{$s_0$}
    \State Let $v_0$ be the root of the MCTS tree, with $v_0.state = s_0$.
    \While{$U(s_0) - L(s_0) > \epsilon$}
        \State $v_l \gets \Call{TreePolicy}{v_0}$
        \State $\Delta \gets \Call{Rollout}{v_l}$
        \State $\Call{Backup}{v_l, \Delta}$
    \EndWhile
    \State \Return Action from $v_0$ to the best node (by some
    given metric).
\EndFunction

\Function{Backup}{$s_0, s$}
\Repeat
    \State $U(s,a) \coloneqq \sum_{s' \in S} \Delta(s,a)(s')U(s')$
    \State $L(s,a)\, \coloneqq \sum_{s' \in S} \Delta(s,a)(s')L(s')$
    \State $s \gets parent(s)$
\Until{$s \neq s_0$}
\EndFunction
\end{algorithmic}
\end{algorithm}

\section{MCTS-BRTDP}

MCTS-BRTDP is a variation of BMCTS in which each rollout is a
single iteration of BRTDP as described in \autoref{ch_mdp}. This allows
for significantly faster updates as compared to BMCTS but still
utilizing exploration which makes it possible to overcome parts of the
MDP which are hard for BRTDP.

The functions \textsc{TreePolicy} and \textsc{Backup} are implemented in
the same way as in \autoref{alg_bmcts}. Furthermore a map from states to
lists of nodes in the tree is maintained and each time a MEC is
collapsed, the subtrees in the MCTS tree corresponding to the nodes
representing a state in the MEC are removed.

This ensures that the algorithm maintains a consistent representation of
the MDP throughout the computation. Since MCTS does not restrict BRTDP
access to any part of the MDP it follows that the algorithm almost
surely converges to the correct value.

\begin{algorithm}
\caption{MCTS-BRTDP}
\label{mcts-brtdp}
\begin{algorithmic}
\Function{MCTS-BRTDP}{$s_0$}
    \State Let $v_0$ be the root of the MCTS tree, with $v_0.state = s_0$.
    \While{$U(s_0) - L(s_0) > \epsilon$}
        \State $v_l \gets \Call{TreePolicy}{v_0}$
        \State $\Delta \gets \text{an iteration of } \Call{BRTDP}{v_l}$
        \State $\Call{Backup}{v_l, \Delta}$
    \EndWhile
    \State \Return $(L(s_0), U(s_0))$
\EndFunction

\end{algorithmic}
\end{algorithm}

\section{UCB in BRTDP}

We further present one algorithm which is not based on MCTS but
incorporates the $UCB$ exploration term into BRTDP. Instead of choosing
an action with the greatest upper bound, it chooses the action with the
greatest UCB value.

This algorithm eventually explores as much as the previously mentioned
variants of BRTDP if necessary.

\chapter{MCTS in MDP Verification}

In this chapter three new algorithms for verification of Markov decision
processes are presented. One called Bounded MCTS (BMCTS) follows
closely the general MCTS scheme but the updates maintain lower and upper
bounds. The second, MCTS-BRTDP, is a fusion of MCTS, where each
rollout is a single iteration of BRTDP.  The third one is called
BRTDP-UCB and uses the UCB formula to select the next action in BRTDP.

Before proceeding, recall from \autoref{sec:brtdp} that $L,U : S \times
A \to [0,1]$, where $S, A$ are the sets of states and actions of an MDP,
are the lower and upper bounds of the value function $V$.
The section also defined
$U(s) = \max_{a \in E(s)} U(s, a)$
and
$L(s) = \max_{a \in E(s)} L(s, a)$.

The algorithms we suggest can be altered by changing the tree node
selection heuristic. We suggest and evaluate three formulas: UCB, VCB,
CCB.  Here $n_i$ is the number of visits of the node and $n$ is the
number of iterations (visits of the root node).
\begin{equation*}
    UCB(i) = U(i) + C \sqrt{ \frac{2 \ln n}{n_i} }
\end{equation*}

In the VCB (Victory Confidence Bound), $v_i$ is the number of times a
target state has been hit when rolling out from state $i$ or its child.
However experiments show its performance is similar to UCB in
practice or worse on some examples.
\[
    VCB(i) = \frac{v_i}{n_i} + C \sqrt{ \frac{2 \ln n}{n_i} }
\]

Furthermore we experimented with a formula which would utilize our
confidence in the learned upper bound but found out that on our models
its performance is very bad.
\[
    CCB(i) = U(i) (1 - (U(i) - L(i))) + C \sqrt{ \frac{2 \ln n}{n_i} }
\]

\pagebreak

\section{Bounded MCTS}

BMCTS is an algorithm which implements the general MCTS scheme in the
most straightforward way while guaranteeing approximate correctness of
the result.

Function \textsc{TreePolicy} is implemented as in
\autoref{alg:uct} and uses one of the formulas
described at the beginning of this chapter.
In each step of each rollout the action is chosen
uniformly at random and a state is then sampled according to the
transition distribution (like the HIGH-PROB variant of BRTDP).

The primary distinction is in the backpropagation phase. To maintain the lower
and upper bounds, backpropagation using the Bellman update is performed
first in the MDP (\textsc{Backup}), and then in the tree (\textsc{TreeBackup}).

\textsc{Backup}: Perform Bellman updates from the terminal state reached in the
rollout to the state corresponding to the selected tree node. This is
the same procedure as in BRTDP.

\textsc{TreeBackup}: Perform Bellman updates from the states
corresponding to the nodes on the taken tree path. Here the algorithm
updates the bounds for every action as the tree does not differentiate
between actions.

Compare this backpropagation with a typical MCTS implementation where
this is not necessary and updates are performed only on the tree path.


This algorithm is an improvement upon random sampling as it guides the
random rollouts towards the parts of the decision process which look
more relevant for maximizing the sought probability. However, the random
rollouts prove to be ineffective in practice, which leads us to our main
algorithm.

\newpage

\begin{algorithm}[H]
\caption{BMCTS}
\label{alg_bmcts}
\begin{algorithmic}
\Function{BMCTS}{$s_0$}
    \State Let $v_0$ be the root of the MCTS tree, with $v_0.state = s_0$.
    \While{$U(s_0) - L(s_0) > \epsilon$}
        \State $v_l \gets \Call{TreePolicy}{v_0}$
        \State $\omega \gets \Call{Rollout}{v_l.state}$
        \State $\Call{Backup}{\omega}$
        \State $\Call{TreeBackup}{v_l}$
    \EndWhile
    \State \Return $(L(s_0), U(s_0))$
\EndFunction

\Function{Rollout}{s}
    \State $\omega \gets s$
    \While{$last(\omega)$ is not terminal}
        \State $a \gets$ sample uniformly from $E(s)$
        \State $s \gets$ sample according to $\Delta(last(\omega),a)$
        \State $\omega \gets \omega \; a \; s$
    \EndWhile
    \State \Return $\omega$
\EndFunction

\Function{Backup}{$\omega$}
\While{$\omega$ is not empty}
    \State $pop(\omega)$
    \State $a \gets pop(\omega)$
    \State $s \gets pop(\omega)$
    \State $U(s,a) \coloneqq \sum_{s' \in S} \Delta(s,a)(s')U(s')$
    \State $L(s,a)\, \coloneqq \sum_{s' \in S} \Delta(s,a)(s')L(s')$
\EndWhile
\EndFunction

\Function{TreeBackup}{$v$}
\State $v \gets v.parent$ \Comment{The leaf's update was done in \textsc{Backup}}
\While{$v \neq v_0$}
    \State $s \gets v.state$
    \For{$a \in E(s)$}
        \State $U(s,a) \coloneqq \sum_{s' \in S} \Delta(s,a)(s')U(s')$
        \State $L(s,a)\, \coloneqq \sum_{s' \in S} \Delta(s,a)(s')L(s')$
    \EndFor
    \State $v \gets v.parent$
\EndWhile
\EndFunction
\end{algorithmic}
\end{algorithm}


\section{MCTS-BRTDP}

MCTS-BRTDP is a variation of BMCTS in which each rollout is a
single iteration of BRTDP as described in \autoref{ch_mdp}. This allows
for significantly faster updates of the bounds as compared to BMCTS but
still utilizing exploration which makes it possible to overcome parts of
the MDP which are hard for BRTDP.

The functions \textsc{TreePolicy} and \textsc{TreeBackup} are implemented in
the same way as in \autoref{alg_bmcts}. Note that we avoid using
\textsc{Backup} as that is done by BRTDP.

The implementation of BRTDP used here maintains a map from states to
lists of nodes in the tree is maintained and each time a MEC is
collapsed, the subtrees in the MCTS tree corresponding to the nodes
representing a state in the MEC are removed.

This ensures that the algorithm maintains a consistent representation of
the MDP throughout the computation. Since MCTS does not restrict BRTDP
access to any part of the MDP it follows that the algorithm almost
surely converges to the correct value.

\begin{algorithm}
\caption{MCTS-BRTDP}
\label{mcts-brtdp}
\begin{algorithmic}
\Function{MCTS-BRTDP}{$s_0$}
    \State Let $v_0$ be the root of the MCTS tree, with $v_0.state = s_0$.
    \While{$U(s_0) - L(s_0) > \epsilon$}
        \State $v_l \gets \Call{TreePolicy}{v_0}$
        \State an iteration of $\Call{BRTDP}{v_l}$
        \State $\Call{TreeBackup}{v_l}$
    \EndWhile
    \State \Return $(L(s_0), U(s_0))$
\EndFunction

\end{algorithmic}
\end{algorithm}

\section{UCB in BRTDP}

We further present one algorithm which is not based on MCTS but
incorporates the $UCB$ exploration term into BRTDP. Instead of choosing
an action with the greatest upper bound, it chooses the action with the
greatest UCB value.

This algorithm eventually explores as much as the previously mentioned
variants of BRTDP if necessary.
